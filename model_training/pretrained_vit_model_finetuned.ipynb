{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a0d6b6",
   "metadata": {},
   "source": [
    "                                                        1️ Imports & Global Config                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "086cfbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# HuggingFace ViT\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "\n",
    "# Metrics & utils\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632bc57",
   "metadata": {},
   "source": [
    "                                                        2️ Device & Reproducibility                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "449165fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456ebb9",
   "metadata": {},
   "source": [
    "                                                        3️ Training Configuration                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5                    # Increased slightly from 3 to 5\n",
    "LR = 5e-6                     # Lower LR\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "\n",
    "TRAIN_DIR = \"dataset/real-vs-fake/train\"\n",
    "VALID_DIR = \"dataset/real-vs-fake/valid\"\n",
    "TEST_DIR  = \"dataset/real-vs-fake/test\"\n",
    "\n",
    "MODEL_OUTPUT = \"model/pretrained_vit_model_finetuned.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebffe00",
   "metadata": {},
   "source": [
    "                                                    4️ Transforms (MATCH INFERENCE EXACTLY)                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5bdef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(0.2, 0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b53b0",
   "metadata": {},
   "source": [
    "                                                        5️ Datasets & Loaders                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfa76f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'fake': 0, 'real': 1}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(TRAIN_DIR, train_transform)\n",
    "valid_dataset = datasets.ImageFolder(VALID_DIR, val_transform)\n",
    "test_dataset  = datasets.ImageFolder(TEST_DIR,  val_transform)\n",
    "\n",
    "print(\"Class mapping:\", train_dataset.class_to_idx)\n",
    "# {'fake': 0, 'real': 1}\n",
    "\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a76de",
   "metadata": {},
   "source": [
    "                                                6️ Class Weights (IMPORTANT – Even if Balanced)                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f7d579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts: Counter({0: 50000, 1: 50000})\n",
      "Class weights: tensor([2., 2.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter([label for _, label in train_dataset.samples])\n",
    "print(\"Train class counts:\", label_counts)\n",
    "\n",
    "total = sum(label_counts.values())\n",
    "weights = [\n",
    "    total / label_counts[0],   # fake\n",
    "    total / label_counts[1],   # real\n",
    "]\n",
    "\n",
    "class_weights = torch.tensor(weights).to(DEVICE)\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f8c27",
   "metadata": {},
   "source": [
    "                                                    7️ Load ViT & Fine-Tuning Strategy                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a88f213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = ViTConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = NUM_CLASSES\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "\n",
    "# Freeze everything\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Unfreeze LAST 3 transformer blocks\n",
    "for p in model.vit.encoder.layer[-3:].parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Unfreeze classifier\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e9da3",
   "metadata": {},
   "source": [
    "                                                8️ Optimizer, Loss (WITH LABEL SMOOTHING)                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "890c9f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariharan.K.C\\AppData\\Local\\Temp\\ipykernel_17740\\862018220.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LR\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=class_weights,\n",
    "    label_smoothing=0.1        # CRITICAL for fine-tuning\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481082ee",
   "metadata": {},
   "source": [
    "                                                    9️ Training Loop (Stable & Correct)                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f927a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/6250 [00:00<?, ?it/s]C:\\Users\\Ariharan.K.C\\AppData\\Local\\Temp\\ipykernel_17740\\1769396591.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
      "Epoch 1/5: 100%|██████████| 6250/6250 [16:48<00:00,  6.20it/s, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.4149 | Val Acc: 0.9338 | Avg Conf: 0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   0%|          | 0/6250 [00:00<?, ?it/s]C:\\Users\\Ariharan.K.C\\AppData\\Local\\Temp\\ipykernel_17740\\1769396591.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
      "Epoch 2/5: 100%|██████████| 6250/6250 [13:17<00:00,  7.84it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.3004 | Val Acc: 0.9563 | Avg Conf: 0.8941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   0%|          | 0/6250 [00:00<?, ?it/s]C:\\Users\\Ariharan.K.C\\AppData\\Local\\Temp\\ipykernel_17740\\1769396591.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
      "Epoch 3/5: 100%|██████████| 6250/6250 [11:21<00:00,  9.17it/s, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.2708 | Val Acc: 0.9673 | Avg Conf: 0.9125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   0%|          | 0/6250 [00:00<?, ?it/s]C:\\Users\\Ariharan.K.C\\AppData\\Local\\Temp\\ipykernel_17740\\1769396591.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
      "Epoch 4/5:   9%|▉         | 576/6250 [01:10<11:34,  8.17it/s, loss=0.21]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[0;32m     14\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\ZT\\Thuliyam AI\\thuliyam_AI\\model_training\\.venv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32md:\\ZT\\Thuliyam AI\\thuliyam_AI\\model_training\\.venv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32md:\\ZT\\Thuliyam AI\\thuliyam_AI\\model_training\\.venv\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
    "            logits = model(images).logits\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    preds, trues, confs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            logits = model(images).logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            conf, pred = torch.max(probs, 1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            trues.extend(labels.cpu().numpy())\n",
    "            confs.extend(conf.cpu().numpy())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1} | \"\n",
    "        f\"Train Loss: {running_loss/len(train_loader):.4f} | \"\n",
    "        f\"Val Acc: {accuracy_score(trues, preds):.4f} | \"\n",
    "        f\"Avg Conf: {np.mean(confs):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0665e",
   "metadata": {},
   "source": [
    "                                            10 Test Evaluation + Confusion Matrix (MANDATORY CHECK)                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62811656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.96995\n",
      "Confusion Matrix:\n",
      "[[9714  286]\n",
      " [ 315 9685]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        logits = model(images.to(DEVICE)).logits\n",
    "        test_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "        test_trues.extend(labels.numpy())\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(test_trues, test_preds))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_trues, test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d24a5",
   "metadata": {},
   "source": [
    "                                                        1️1 Save Model (.pt)                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7a5f723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 model saved at: pretrained_vit_model_finetuned.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": 3,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"class_mapping\": {\"fake\": 0, \"real\": 1},\n",
    "        \"architecture\": \"vit-base-patch16-224\",\n",
    "        \"val_accuracy\": 0.9673\n",
    "    },\n",
    "    MODEL_OUTPUT\n",
    ")\n",
    "\n",
    "print(\"Epoch 3 model saved at:\", MODEL_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7013c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_OUTPUT)\n",
    "print(\"Saved model to:\", MODEL_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd596ca7",
   "metadata": {},
   "source": [
    "                                                        1️2️ Verify Model Load                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e90f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(MODEL_OUTPUT, map_location=\"cpu\"))\n",
    "loaded_model.eval()\n",
    "print(\"PT model loaded successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
